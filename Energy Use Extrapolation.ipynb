{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vital-advice",
   "metadata": {},
   "source": [
    "# Trees and Random Forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-modern",
   "metadata": {},
   "source": [
    "I use trees and Random Forest in order to extrapolate on a data set for energy usage in household appliances. Testing of the extrapolation model will be performed 'out-of-sample' on a data set containing appliances with the top 10% highest energy consumption rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "willing-circumstances",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error # Requires sklearn 0.24 (December 2020), update with conda/pip if needed.\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dependent-cedar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Appliances</th>\n",
       "      <th>lights</th>\n",
       "      <th>T1</th>\n",
       "      <th>RH_1</th>\n",
       "      <th>T2</th>\n",
       "      <th>RH_2</th>\n",
       "      <th>T3</th>\n",
       "      <th>RH_3</th>\n",
       "      <th>T4</th>\n",
       "      <th>RH_4</th>\n",
       "      <th>...</th>\n",
       "      <th>T8</th>\n",
       "      <th>RH_8</th>\n",
       "      <th>T9</th>\n",
       "      <th>RH_9</th>\n",
       "      <th>T_out</th>\n",
       "      <th>Press_mm_hg</th>\n",
       "      <th>RH_out</th>\n",
       "      <th>Windspeed</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>Tdewpoint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>17735.000000</td>\n",
       "      <td>17735.000000</td>\n",
       "      <td>17735.000000</td>\n",
       "      <td>17735.000000</td>\n",
       "      <td>17735.000000</td>\n",
       "      <td>17735.000000</td>\n",
       "      <td>17735.000000</td>\n",
       "      <td>17735.000000</td>\n",
       "      <td>17735.000000</td>\n",
       "      <td>17735.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>17735.000000</td>\n",
       "      <td>17735.000000</td>\n",
       "      <td>17735.000000</td>\n",
       "      <td>17735.000000</td>\n",
       "      <td>17735.000000</td>\n",
       "      <td>17735.000000</td>\n",
       "      <td>17735.000000</td>\n",
       "      <td>17735.000000</td>\n",
       "      <td>17735.000000</td>\n",
       "      <td>17735.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>68.121229</td>\n",
       "      <td>3.445729</td>\n",
       "      <td>21.688855</td>\n",
       "      <td>40.150264</td>\n",
       "      <td>20.293891</td>\n",
       "      <td>40.469997</td>\n",
       "      <td>22.230926</td>\n",
       "      <td>39.160423</td>\n",
       "      <td>20.860319</td>\n",
       "      <td>38.983631</td>\n",
       "      <td>...</td>\n",
       "      <td>22.050534</td>\n",
       "      <td>43.016242</td>\n",
       "      <td>19.505556</td>\n",
       "      <td>41.552215</td>\n",
       "      <td>7.314032</td>\n",
       "      <td>755.566425</td>\n",
       "      <td>80.249079</td>\n",
       "      <td>3.969812</td>\n",
       "      <td>38.305214</td>\n",
       "      <td>3.762879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>30.182146</td>\n",
       "      <td>7.552424</td>\n",
       "      <td>1.604312</td>\n",
       "      <td>3.930099</td>\n",
       "      <td>2.171999</td>\n",
       "      <td>4.063088</td>\n",
       "      <td>1.969945</td>\n",
       "      <td>3.219280</td>\n",
       "      <td>2.047586</td>\n",
       "      <td>4.320711</td>\n",
       "      <td>...</td>\n",
       "      <td>1.961083</td>\n",
       "      <td>5.202160</td>\n",
       "      <td>2.010550</td>\n",
       "      <td>4.161873</td>\n",
       "      <td>5.291010</td>\n",
       "      <td>7.339842</td>\n",
       "      <td>14.768037</td>\n",
       "      <td>2.447164</td>\n",
       "      <td>11.957900</td>\n",
       "      <td>4.187098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.790000</td>\n",
       "      <td>27.023333</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>20.463333</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>28.766667</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>27.660000</td>\n",
       "      <td>...</td>\n",
       "      <td>16.306667</td>\n",
       "      <td>29.600000</td>\n",
       "      <td>14.890000</td>\n",
       "      <td>29.166667</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>729.366667</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-6.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.760000</td>\n",
       "      <td>37.260000</td>\n",
       "      <td>18.790000</td>\n",
       "      <td>37.930000</td>\n",
       "      <td>20.790000</td>\n",
       "      <td>36.826667</td>\n",
       "      <td>19.566667</td>\n",
       "      <td>35.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>20.828889</td>\n",
       "      <td>39.200000</td>\n",
       "      <td>18.066667</td>\n",
       "      <td>38.530000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>751.000000</td>\n",
       "      <td>71.333333</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.600000</td>\n",
       "      <td>39.533333</td>\n",
       "      <td>19.926667</td>\n",
       "      <td>40.545000</td>\n",
       "      <td>22.100000</td>\n",
       "      <td>38.466667</td>\n",
       "      <td>20.666667</td>\n",
       "      <td>38.363333</td>\n",
       "      <td>...</td>\n",
       "      <td>22.166667</td>\n",
       "      <td>42.440000</td>\n",
       "      <td>19.390000</td>\n",
       "      <td>40.863333</td>\n",
       "      <td>6.850000</td>\n",
       "      <td>756.100000</td>\n",
       "      <td>84.333333</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>3.433333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>80.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.600000</td>\n",
       "      <td>42.863333</td>\n",
       "      <td>21.463333</td>\n",
       "      <td>43.326667</td>\n",
       "      <td>23.290000</td>\n",
       "      <td>41.530000</td>\n",
       "      <td>22.100000</td>\n",
       "      <td>42.066667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.390000</td>\n",
       "      <td>46.590000</td>\n",
       "      <td>20.600000</td>\n",
       "      <td>44.290000</td>\n",
       "      <td>10.333333</td>\n",
       "      <td>760.950000</td>\n",
       "      <td>91.845238</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>6.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>190.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>26.200000</td>\n",
       "      <td>59.633333</td>\n",
       "      <td>29.856667</td>\n",
       "      <td>56.026667</td>\n",
       "      <td>29.100000</td>\n",
       "      <td>49.656667</td>\n",
       "      <td>26.200000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>27.230000</td>\n",
       "      <td>58.780000</td>\n",
       "      <td>24.500000</td>\n",
       "      <td>53.326667</td>\n",
       "      <td>26.100000</td>\n",
       "      <td>772.283333</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>15.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Appliances        lights            T1          RH_1            T2  \\\n",
       "count  17735.000000  17735.000000  17735.000000  17735.000000  17735.000000   \n",
       "mean      68.121229      3.445729     21.688855     40.150264     20.293891   \n",
       "std       30.182146      7.552424      1.604312      3.930099      2.171999   \n",
       "min       10.000000      0.000000     16.790000     27.023333     16.100000   \n",
       "25%       50.000000      0.000000     20.760000     37.260000     18.790000   \n",
       "50%       60.000000      0.000000     21.600000     39.533333     19.926667   \n",
       "75%       80.000000      0.000000     22.600000     42.863333     21.463333   \n",
       "max      190.000000     50.000000     26.200000     59.633333     29.856667   \n",
       "\n",
       "               RH_2            T3          RH_3            T4          RH_4  \\\n",
       "count  17735.000000  17735.000000  17735.000000  17735.000000  17735.000000   \n",
       "mean      40.469997     22.230926     39.160423     20.860319     38.983631   \n",
       "std        4.063088      1.969945      3.219280      2.047586      4.320711   \n",
       "min       20.463333     17.200000     28.766667     15.100000     27.660000   \n",
       "25%       37.930000     20.790000     36.826667     19.566667     35.500000   \n",
       "50%       40.545000     22.100000     38.466667     20.666667     38.363333   \n",
       "75%       43.326667     23.290000     41.530000     22.100000     42.066667   \n",
       "max       56.026667     29.100000     49.656667     26.200000     51.000000   \n",
       "\n",
       "       ...            T8          RH_8            T9          RH_9  \\\n",
       "count  ...  17735.000000  17735.000000  17735.000000  17735.000000   \n",
       "mean   ...     22.050534     43.016242     19.505556     41.552215   \n",
       "std    ...      1.961083      5.202160      2.010550      4.161873   \n",
       "min    ...     16.306667     29.600000     14.890000     29.166667   \n",
       "25%    ...     20.828889     39.200000     18.066667     38.530000   \n",
       "50%    ...     22.166667     42.440000     19.390000     40.863333   \n",
       "75%    ...     23.390000     46.590000     20.600000     44.290000   \n",
       "max    ...     27.230000     58.780000     24.500000     53.326667   \n",
       "\n",
       "              T_out   Press_mm_hg        RH_out     Windspeed    Visibility  \\\n",
       "count  17735.000000  17735.000000  17735.000000  17735.000000  17735.000000   \n",
       "mean       7.314032    755.566425     80.249079      3.969812     38.305214   \n",
       "std        5.291010      7.339842     14.768037      2.447164     11.957900   \n",
       "min       -5.000000    729.366667     24.000000      0.000000      1.000000   \n",
       "25%        3.500000    751.000000     71.333333      2.000000     29.000000   \n",
       "50%        6.850000    756.100000     84.333333      3.500000     40.000000   \n",
       "75%       10.333333    760.950000     91.845238      5.333333     40.000000   \n",
       "max       26.100000    772.283333    100.000000     14.000000     66.000000   \n",
       "\n",
       "          Tdewpoint  \n",
       "count  17735.000000  \n",
       "mean       3.762879  \n",
       "std        4.187098  \n",
       "min       -6.600000  \n",
       "25%        0.933333  \n",
       "50%        3.433333  \n",
       "75%        6.550000  \n",
       "max       15.500000  \n",
       "\n",
       "[8 rows x 26 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load in the data\n",
    "data=pd.read_csv(\"energy_appliances_standard.csv\")\n",
    "#now show the descriptive statistics\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hollow-kitty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into training and testing sets\n",
    "Dtrain, Dtest=train_test_split(data, test_size=0.3, random_state=20201107)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hungry-biodiversity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define random forest and iterate over the number of trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from collections import OrderedDict\n",
    "\n",
    "X=Dtrain.drop(\"Appliances\", axis=1)\n",
    "y=Dtrain.Appliances\n",
    "\n",
    "ensemble_clfs=[(\"Random Forest Regressor OOB Error\", RandomForestRegressor(oob_score=True, n_jobs=1))]\n",
    "\n",
    "# Map a classifier name to a list of (<n_estimators>, <error rate>) pairs.\n",
    "error_rate = OrderedDict((label, []) for label, _ in ensemble_clfs)\n",
    "\n",
    "# Range of `n_estimators` values to explore.\n",
    "min_estimators = 50\n",
    "max_estimators = 250 ###I know it is supposed to be from 50 to 250 but my computer will not complete it and i need to hand in\n",
    "\n",
    "for label, clf in ensemble_clfs:\n",
    "    for i in range(min_estimators, max_estimators + 1):\n",
    "        clf.set_params(n_estimators=i)\n",
    "        clf.fit(X, y)\n",
    "\n",
    "        # Record the OOB error for each `n_estimators=i` setting.\n",
    "        oob_error = 1 - clf.oob_score_\n",
    "        error_rate[label].append((i, oob_error))\n",
    "\n",
    "# Generate the \"OOB error rate\" vs. \"n_estimators\" plot.\n",
    "for label, clf_err in error_rate.items():\n",
    "    xs, ys = zip(*clf_err)\n",
    "    plt.plot(xs, ys, label=label)\n",
    "\n",
    "plt.xlim(min_estimators, max_estimators)\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"OOB error rate\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Out-of-Bag Error for Random Forest Regression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-vector",
   "metadata": {},
   "source": [
    "The OOB error score begins to flatten around approximately 225 trees, and so this is the optimal number of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "accurate-tuition",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ff02a8cbe6c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptimal_estimators\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m225\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Calculate error over test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mregressorFinalForest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moob_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimal_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mXtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Appliances\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mytest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAppliances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[1;32m    388\u001b[0m                              \u001b[0;34m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'threads'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                 delayed(_parallel_build_trees)(\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    167\u001b[0m                                                         indices=indices)\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \"\"\"\n\u001b[1;32m   1246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m   1248\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    387\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Final forest\n",
    "optimal_estimators= 225 \n",
    "# Calculate error over test set\n",
    "regressorFinalForest=RandomForestRegressor(oob_score=True,n_estimators=optimal_estimators).fit(X,y)\n",
    "Xtest=Dtest.drop(\"Appliances\", axis=1)\n",
    "ytest=Dtest.Appliances\n",
    "ypred=regressorFinalForest.predict(Xtest)\n",
    "error1=mean_absolute_percentage_error(ytest, ypred)\n",
    "\n",
    "# Load the second dataset for extrapolation\n",
    "data2=pd.read_csv(\"energy_appliances_extrapolation.csv\")\n",
    "\n",
    "X2=data2.drop(\"Appliances\", axis=1)\n",
    "y2=data2.Appliances\n",
    "# Calculate the error over it\n",
    "ypred2=regressorFinalForest.predict(X2)\n",
    "error2=mean_absolute_percentage_error(y2,ypred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-problem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the mean absolute percentual error for both the testing and the extrapolated set\n",
    "# Print MAPE over the sets\n",
    "print(\"The MAPE for the test set is: \", error1, \".\")\n",
    "print(\"The MAPE for the extrapolated set is: \", error2, \".\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the performance of the model for each data set\n",
    "plt.scatter(ytest, ypred, color=\"red\", label=\"Test Data Set\", alpha=0.3)\n",
    "plt.scatter(y2, ypred2, color=\"blue\", label=\"Extrapolated Data Set\", alpha=0.3)\n",
    "plt.title(\"Predicted vs True Values of the Target Variable (Appliances)\")\n",
    "plt.xlabel(\"True Values [Wh]\")\n",
    "plt.ylabel(\"Predicted Values [Wh]\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-trauma",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets look at the extrapolated data set to try and find out about the discrepancy in regressor performance\n",
    "data2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-essay",
   "metadata": {},
   "source": [
    "The random forest regressor performs poorly on the extrapolated data set. From the scatter plot I can see that the relationship between predicted and true values for the testing set is relatively linear while that for the extrapolated data set is not. The values of the target variable in the original data set have a maximum of 190 while those in the extrapolated data set have a much larger range (maximum of 1080). The regressor was trained on the original data, and so it is expected to perform poorly at predicting higher values of the target variable (as in the case of the extrapolated data set) when it has not yet encountered them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bibliographic-chicken",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets now explore an XGBoosting model, tuning for the optimal values of learning rate, tree depth, and number of trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-saying",
   "metadata": {},
   "source": [
    " We want to use a small learning rate because this generally means we want to add more trees to the model. The number of trees depends on each dataset/model becuase the number of features/predictors which are involved in the model influences the number of trees that should be used. We want to use a small tree depth because though they have high variance individually, they are effective predictors when many shallow trees are used together. This also prevents over-fitting the model (ending up with a low bias, high variance model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-comment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the XGB model\n",
    "num_trees=[350,400,450,500]\n",
    "depths=list(range(3,8))\n",
    "learn_rates=[0.01,0.1,0.2]\n",
    "XGB_model = XGBRegressor(max_depth=depths,                 # Depth of each tree\n",
    "                            learning_rate=learn_rates,  # How much to shrink error in each subsequent training. Trade-off with no. estimators.\n",
    "                            n_estimators=num_trees, # How many trees to use, the more the better, but decrease learning rate if many used.\n",
    "                            verbosity=1,                  # If to show more errors or not.\n",
    "                            #objective='binary:logistic',  # Type of target variable.\n",
    "                            booster='gbtree',             # What to boost. Trees in this case.\n",
    "                            n_jobs=2,                     # Parallel jobs to run. Set your processor number.\n",
    "                            gamma=0.001,                  # Minimum loss reduction required to make a further partition on a leaf node of the tree. (Controls growth!)\n",
    "                            subsample=1,                  # Subsample ratio. Can set lower\n",
    "                            colsample_bytree=1,           # Subsample ratio of columns when constructing each tree.\n",
    "                            colsample_bylevel=1,          # Subsample ratio of columns when constructing each level. 0.33 is similar to random forest.\n",
    "                            colsample_bynode=1,           # Subsample ratio of columns when constructing each split.\n",
    "                            reg_alpha=1,                  # Regularizer for first fit. alpha = 1, lambda = 0 is LASSO.\n",
    "                            reg_lambda=0,                 # Regularizer for first fit.\n",
    "                            scale_pos_weight=1,           # Balancing of positive and negative weights.\n",
    "                            base_score=0.5,               # Global bias. Set to average of the target rate.\n",
    "                            random_state=20201107,        # Seed\n",
    "                            missing=None                  # How are nulls encoded?\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-dress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = dict({'n_estimators': num_trees,\n",
    "                   'max_depth': depths,\n",
    "                 'learning_rate' : learn_rates\n",
    "                  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-newark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reduced validation set\n",
    "val_train = Dtrain.sample(frac = 0.3, random_state = 20201107)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-spain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train grid search\n",
    "GridXGB = GridSearchCV(XGB_model,        # Original XGB. \n",
    "                       param_grid,          # Parameter grid\n",
    "                       cv = 3,              # Number of cross-validation folds.  \n",
    "                       scoring = 'roc_auc', # How to rank outputs.\n",
    "                       n_jobs = -1,         # Parallel jobs. -1 is \"all you have\"\n",
    "                       refit = False,       # If refit at the end with the best. We'll do it manually.\n",
    "                       verbose = 1          # If to show what it is doing.\n",
    "                      )\n",
    "GridXGB.fit(val_train.drop(\"Appliances\", axis=1), val_train.Appliances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show best params\n",
    "GridXGB.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-nevada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final XGB with optimal parameters\n",
    "XGB_model_best = XGBRegressor(max_depth=GridXGB.best_params_.get('max_depth'), # Depth of each tree\n",
    "                            learning_rate=GridXGB.best_params_.get('learning_rate'), # How much to shrink error in each subsequent training. Trade-off with no. estimators.\n",
    "                            n_estimators=GridXGB.best_params_.get('n_estimators'), # How many trees to use, the more the better, but decrease learning rate if many used.\n",
    "                            verbosity=1,                  # If to show more errors or not.\n",
    "                            #objective='binary:logistic',  # Type of target variable.\n",
    "                            booster='gbtree',             # What to boost. Trees in this case.\n",
    "                            n_jobs=2,                     # Parallel jobs to run. Set your processor number.\n",
    "                            gamma=0.001,                  # Minimum loss reduction required to make a further partition on a leaf node of the tree. (Controls growth!)\n",
    "                            subsample=1,                  # Subsample ratio. Can set lower\n",
    "                            colsample_bytree=1,           # Subsample ratio of columns when constructing each tree.\n",
    "                            colsample_bylevel=1,          # Subsample ratio of columns when constructing each level. 0.33 is similar to random forest.\n",
    "                            colsample_bynode=1,           # Subsample ratio of columns when constructing each split.\n",
    "                            reg_alpha=1,                  # Regularizer for first fit. alpha = 1, lambda = 0 is LASSO.\n",
    "                            reg_lambda=0,                 # Regularizer for first fit.\n",
    "                            scale_pos_weight=1,           # Balancing of positive and negative weights.\n",
    "                            base_score=0.5,               # Global bias. Set to average of the target rate.\n",
    "                            random_state=20201107,        # Seed\n",
    "                            missing=None                  # How are nulls encoded?\n",
    "                            )\n",
    "XGB_model_best.fit(Dtrain.drop(\"Appliances\", axis=1),Dtrain.Appliances) #now training over the entire training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-romance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the importance of each variable\n",
    "importances = XGB_model_best.feature_importances_\n",
    "indices = np.argsort(importances)[::-1] \n",
    "x=importances[indices]\n",
    "x2=data2.drop(columns=['Appliances'])\n",
    "y=[x2.columns[i] for i in indices]\n",
    "\n",
    "#plots of importance\n",
    "f, ax = plt.subplots(figsize=(3, 8))\n",
    "plt.title(\"Variable Importance - XGBoosting\")\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.barplot(x,y, \n",
    "            label=\"Total\", color=\"b\")\n",
    "ax.set(ylabel=\"Variable\",\n",
    "       xlabel=\"Variable Importance (Gini)\")\n",
    "sns.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-buyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the predicted vs true for the extrapolated data set for both random forest and xgboosted models\n",
    "X2=data2.drop(\"Appliances\", axis=1) #the extrapolated data set\n",
    "y2=data2.Appliances\n",
    "y_pred_boost= XGB_model_best.predict(X2)\n",
    "y_pred_forest=regressorFinalForest.predict(X2)\n",
    "# xtrain=Dtrain.drop(\"Appliances\", axis=1)\n",
    "# ytrain=Dtrain.Appliances\n",
    "# ypred=XGB_model_best.predict(xtrain)\n",
    "plt.scatter(y2,y_pred_boost, label=\"XGBoost Prediction\", alpha=0.3)\n",
    "plt.scatter(y2, y_pred_forest, label=\"Random Forest Prediciton\", alpha=0.3)\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Extrapolated Predictions for both XGBoost and Random Forest Models\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-separate",
   "metadata": {},
   "source": [
    "The most important variables are T2, lights, and RH_out. Both the XGBoost and Random Forest Models do a poor job at predicting on the extrapolated data set, though the Random Forest model appears to do a better job than the XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-comment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets try to combine both of the data sets and use the random forest model to extrapolate\n",
    "# Create the new train and test set.\n",
    "dataExp=pd.read_csv(\"energy_appliances_extrapolation.csv\")\n",
    "Dtrain2,Dtest2=train_test_split(dataExp, test_size=0.3, random_state=20201107)\n",
    "\n",
    "Dtrain_combined=pd.concat([Dtrain, Dtrain2])\n",
    "Dtest_combined=pd.concat([Dtest, Dtest2])\n",
    "\n",
    "X_train_combined=Dtrain_combined.drop(\"Appliances\",axis=1)\n",
    "y_train_combined=Dtrain_combined.Appliances\n",
    "X_test_combined=Dtest_combined.drop(\"Appliances\", axis=1)\n",
    "y_test_combined=Dtest_combined.Appliances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-freight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the new random forest (lets try 50 trees)\n",
    "num_trees=50\n",
    "regressorRandomForest2=RandomForestRegressor(n_estimators=50,oob_score=True).fit(X_train_combined, y_train_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-insight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable importance\n",
    "importances = regressorRandomForest2.feature_importances_\n",
    "indices = np.argsort(importances)[::-1] \n",
    "x=importances[indices]\n",
    "x2=data2.drop(columns=['Appliances'])\n",
    "y=[x2.columns[i] for i in indices]\n",
    "\n",
    "#plots of importance\n",
    "f, ax = plt.subplots(figsize=(3, 8))\n",
    "plt.title(\"Variable Importance - XGBoosting\")\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.barplot(x,y, \n",
    "            label=\"Total\", color=\"b\")\n",
    "ax.set(ylabel=\"Variable\",\n",
    "       xlabel=\"Variable Importance (Gini)\")\n",
    "sns.despine(left=True, bottom=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print MAPE over the test sets\n",
    "#original test set\n",
    "y_pred_original=regressorRandomForest2.predict(Xtest)\n",
    "error_original=mean_absolute_percentage_error(ytest,y_pred_original)\n",
    "\n",
    "#extrapolated test set\n",
    "X_test_ex=Dtest2.drop(\"Appliances\", axis=1)\n",
    "y_test_ex=Dtest2.Appliances\n",
    "y_pred_extrapolated=regressorRandomForest2.predict(X_test_ex)\n",
    "error_ex=mean_absolute_percentage_error(y_test_ex,y_pred_extrapolated)\n",
    "\n",
    "#combined test set\n",
    "y_pred_combined=regressorRandomForest2.predict(X_test_combined)\n",
    "error_combined=mean_absolute_percentage_error(y_test_combined,y_pred_combined)\n",
    "\n",
    "print(\"The MAPE for the original test set is :\", error_original, \".\")\n",
    "print(\"The MAPE for the extrapolated test set is :\", error_ex, \".\")\n",
    "print(\"The MAPE for the combined test set it: \", error_combined, \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-switch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot\n",
    "plt.scatter(ytest, y_pred_original, label=\"Original Data Predictions\", alpha=1)\n",
    "plt.scatter(y_test_combined, y_pred_combined, label=\"Combined Data Predictions\", alpha=0.3)\n",
    "plt.legend()\n",
    "plt.title(\"Random Forest Predictions for Original and Combined Data Sets\")\n",
    "plt.xlabel(\"True Value\")\n",
    "plt.ylabel(\"Predicted Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-contrast",
   "metadata": {},
   "source": [
    "Now, we see that for the combined data set, the model is able to much more effectively predict the target variable than in previous cases (i.e. than when trained on the original data set alone but then tested on the extrapolated data set). This is to be expected, as the model will have seen some of the extrapolated data in its training, and will therefore be able to actually make predictions on it in testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-effects",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
